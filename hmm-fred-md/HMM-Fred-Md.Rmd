---
title: "HMM-Fred-md"
author: "Jai Vrat Singh"
date: "02/07/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r warning=FALSE, message=FALSE}
library(xts)
library(PerformanceAnalytics)
library(tseries)
library(forecast)
library(knitr)
library(kableExtra)
library(ggplot2)
```

Set working directory

```{r}
setwd("~/work/github/R-codes/hmm-fred-md")

direct.df <- read.csv(file = "2019-04.csv", header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
direct.df <- direct.df[-1, ] #The first row is transform etc..

#Date is in month/date/year format
direct.df$sasdate <- as.Date(direct.df$sasdate, "%m/%d/%Y")

#get into time series as it will be earier to work with
direct.df.xts <- xts(direct.df[,-1], direct.df$sasdate)

#Replace missing values by the latest observation. Please note that there may be missing values in "real" decision scenarios which may not be 
#caught by one time experiment. ie in case if we are doing real backtests
direct.df.xts <- zoo::na.locf(direct.df.xts)
```


```{r}
plot.xts(direct.df.xts, legend.loc = "bottomright")
```

As seen from the plot many series seem to ne non-stationary. We need to check for seasonality as well.


##### Let us calculate S&P Returns 
```{r}
sp500.ret <- PerformanceAnalytics::Return.calculate(direct.df.xts$`S&P 500`, method = "log")
plot.xts(sp500.ret, legend.loc = "topleft")
```

We Add another spread based measure/feature to this data set based on 
https://www.newyorkfed.org/medialibrary/media/research/current_issues/ci12-5.pdf

```{r}
bondEqv <- function(discount)
{
  100 * ( 365 * discount/100)/(360 - 91* discount/100)
}
temp.xts <- direct.df.xts$GS10 - bondEqv(as.numeric(direct.df.xts$TB3MS))

old.names <- names(direct.df.xts)
direct.df.xts <- merge.xts(direct.df.xts, temp.xts, all = TRUE, check.names = FALSE)
names(direct.df.xts) <- c(old.names, "GS10_minus_TB3MS")
```


#### Restrict our analysis and feature production purely on train data, ie data till 1996. We should not see anything beyond.
Ideally it should be done on test data only. Because we will apply all these results on completely unseen data.

```{r}
test.direct.df.xts <- direct.df.xts["1997/"]
direct.df.xts      <- direct.df.xts["/1997"]
```


#### Checks for stationarity
````{r}
check.p.val.for.lag <- function(k)
{
  #alternative = "stationary"
  pvalues <- sapply(direct.df.xts, function(in.ts) tseries::adf.test(in.ts[!is.na(in.ts)], k = k)$p.value)
  #if p.vale is less than 0.05, then we reject H0 ie reject that series 
  #is non-stationary, and conclude that it is stationary
  ifelse(pvalues < 0.05, "S", "N")
}

#lags 
lags = 0:30
res  <- suppressWarnings(sapply(lags, function(k) check.p.val.for.lag(k)))
stat.result.direct.df <- data.frame(lags = lags, t(res), check.names = FALSE)
````


```{r}
#Stationary ones with 0 lag
print("Stationary:")
names(stat.result.direct.df[1,-1])[ stat.result.direct.df[1,-1] == "S"]
#Non Stationary
print("Non Stationary")
names(stat.result.direct.df[1,-1])[ stat.result.direct.df[1,-1] == "N"]
```



#### Some Checks on seasonality

```{r}
#There will be error here as data is seasonally adjusted
#plot(decompose(direct.df.xts$RPI))

is.seasonlity <- sapply(names(direct.df.xts), function(name)
                                              {
                                                res <- try(stl(direct.df.xts[, name], s.window = "periodic"),
                                                           silent = TRUE) 
                                                #we do not want to print error
                                                ifelse(class(res) == "try-error", FALSE, TRUE)
                                              })
if(sum(is.seasonlity) > 0)
{
  seasonals <- names(is.seasonlity)[is.seasonlity == TRUE]
  stop(sprintf("These are seasonal ones %s", paste(seasonals, sep= ":", collapse = ":")))
}
```



#### Find differencings to make it stationary 
(**Independt tests on data till 1996 and another till 2018 suggest different lags => this means model needs to be recalibrated after a few years**)
```{r}
ndiffs.suggested <- sapply(direct.df.xts, function(ts.this)
                                          {
                                            forecast::ndiffs(ts.this[!is.na(ts.this)], alpha = 0.05, max.d = 10)
                                          }
                           )
ndiffs.suggested.file <- "ndiffs.suggested.df.csv"
ndiffs.suggested.df <- data.frame("name" = names(ndiffs.suggested) , "lag" = as.numeric(ndiffs.suggested))
write.csv(ndiffs.suggested.df, ndiffs.suggested.file, row.names = FALSE)
```



```{r}
ndiffs.suggested.df %>%
            kable() %>%
            kable_styling()
```

#### Construct stationary series

Make data series stationary. There is  doubt what what to do first, stationarizing or anomaly detection. I think stationarizing first is better as there are not many outliers. Stationarity suggestion is based on overall data, in which outlier will not have any influence. Further differencing sugested will automatically help detect outliers. For simplicity we will not remove outlier but repace them with corresspoding bounds.

```{r}

stationarised.res <- sapply(direct.df.xts, function(this.ts){
                                              lag <- ndiffs.suggested[as.character(names(this.ts))]
                                              #
                                              if(lag == 0){ #no transformation needed
                                                return(this.ts)
                                              } else {
                                                return(diff(this.ts, lag))
                                              }
                                           }, simplify = FALSE)

#Merge this into a stationarised xts object
stationarised.xts <- stationarised.res[[1]]
for(i in 2:length(stationarised.res))
{
  stationarised.xts <- merge.xts(stationarised.xts, stationarised.res[[i]], check.names = FALSE)
}

names(stationarised.xts) <- names(direct.df.xts)

```

*** There are a few series such as NONBORRES which are too voltile after 2008 ***

```{r}
plot.xts(direct.df.xts$NONBORRES)
```

Check later if stationarising worked. (TBD). You can do so by taking recommended lags and doing adf test

tseries::adf.test(stationarised.xts$RPI[!is.na(stationarised.xts$RPI)])

```{r}
#Write stationarised data to a file
write.zoo(stationarised.xts, file = "stationarised.xts.csv", row.names = FALSE)
```


There are lots of NA's, top 4 rows have lots of NA's. Lets remove them first
```{r}
stationarised.xts <- stationarised.xts[-c(1:2), ]
```

```{r}
stationarised.xts.df <- data.frame("dates" = index(stationarised.xts), coredata(stationarised.xts), check.names = FALSE)
```


```{r}
tmp <- sapply(stationarised.xts.df, function(l) sum(is.na(coredata(l))))
ggplot(data = data.frame("variable" = names(tmp), "NAs" = as.numeric(tmp)), aes(x = variable, y = NAs)) + 
  geom_bar(stat= "identity") + 
  ggtitle("Number of NA") + 
  theme(axis.text.x =  element_text(angle  = 90, hjust = 1, size = 6))
```
So there are a few columns which have lots of NA. We can drop them or impute them. Let us try imputation. Same imputation can be used for data metrics which are not avilable while rebalancing.

We will use distance based imputation:

0. Identify outliers and dampen outliers
https://www.chicagofed.org/~/media/publications/cfnai/2002/3qepart2-pdf.pdf

We define an outlier to be an observation whose distance away from the median is greater than six times the interquartile range of the series. That is, $x_{it}$ - the observation at time $t$ of series $i$ is an outlier if $|x_{it} - x_{i}^{50}| > 6(x_{i}^{75} - x_{i}^{25})$,  where $x_{i}^{25}$, $x_{i}^{50}$ and $x_{i}^{75}$ are the 25th, 50th, and 75th percentiles of series $x_i$.

An outlier that is above the median has its original value replaced with $x_{i}^{50} + 6 * (x_{i}^{75} - x_{i}^{25})$, while an outlier that is below the median has its original value replaced with $x_{i}^{50} - 6 * (x_{i}^{75} - x_{i}^{25})$

1. We separate our data into sections - one which has all the data(S1) and other which has some NA's(S2). Then we calculate euclidean distances of each pair of rows. 
2. for each row which has missing data, we find distances of all the rows from this reference row and populate with invesrse distance weighted value.

```{r}
#Outlier detection, let us find 25th, 50th and 75the percentile
pctiles    <- c(0.25, 0.5, 0.75)
pctiles.df <- data.frame("pct" = pctiles, sapply(stationarised.xts, function(l) quantile(l, probs = pctiles, na.rm = TRUE)), check.names = FALSE)
write.csv(x = pctiles.df, file = "pctiles.df.csv", row.names = FALSE)
```


Replace outliers with rules as mentioned above
```{r}
outl.res <- sapply(names(stationarised.xts), function(name) 
                              {
                                x <-  as.numeric(stationarised.xts[, name])
                                qtiles <- pctiles.df[, name]
                                #IQR definition: https://www.mathwords.com/o/outlier.htm
                                
                                #BELOW
                                below.outl    <- (x < qtiles[1] - 3 * (qtiles[3] - qtiles[1]))
                                x[!is.na(x) & below.outl] <- qtiles[1] - 3 * (qtiles[3] - qtiles[1])
                                
                                #Above
                                above.outl    <- (x > qtiles[1] + 3 * (qtiles[3] - qtiles[1]))
                                x[!is.na(x) & above.outl] <- qtiles[1] + 3 * (qtiles[3] - qtiles[1])
                                x
                              })

stationarised.outl.xts <- xts(outl.res, index(stationarised.xts))
names(stationarised.outl.xts) <- names(stationarised.xts)
```

```{r}
#See what changes after outlier handling
plot.xts(stationarised.xts)
plot.xts(stationarised.outl.xts)
```

##### Means and variances of data (after outlier adjustment)

```{r}
means <- apply(stationarised.outl.xts, 2, mean, na.rm = TRUE)
sds   <- apply(stationarised.outl.xts, 2, sd, na.rm = TRUE)
standardization.scale <- data.frame("name" = names(means), means = means, sds = sds)

#Save this to file
write.csv(standardization.scale, "standardization.scale.csv", row.names = FALSE)

#We can also do the same thing above directly in one go 
scaled.dat <- scale(stationarised.outl.xts)

#Scaled statioonarised data frame 
scaled.stnd.df = data.frame("dates" = as.Date(index(scaled.dat)), scaled.dat, check.names = FALSE)
```



##### Let us do the imputation
```{r}
#in.df <- scaled.stnd.df
imput.fun <- function(in.df)
{
  
  #Identify columns which are filled up completely ie no na's
  res.no.na <- sapply( colnames(in.df), function(col.name)
                                        {
                                          vals <- in.df[[col.name]]
                                          ifelse(any(is.na(vals)), FALSE, TRUE)
                                        })
  
  #THose cols without na's
  no.na.cols <- names(res.no.na)[res.no.na]
  
  #THose cols with na's
  na.cols    <- names(res.no.na)[!res.no.na]
  
  df.no.na  <- in.df[, no.na.cols, drop = FALSE]
  
  #To process fast, let as precalculate and cache the distances of rows.
  know.dist <<- matrix(NA, nrow = dim(df.no.na)[1],  ncol = dim(df.no.na)[1])
  
  idxs <- t(combn(1:dim(df.no.na)[1], 2))
  
  system.time( res <- apply(idxs, 1, function(idx){
                                      row.idx1 <- idx[1]; row.idx2 = idx[2]
                                      
                                      know.dist2 <- know.dist[row.idx1, row.idx2]
                                      if(!is.na(know.dist2))
                                      {
                                        return(know.dist2)
                                      }
                                      
                                      this.point <- as.numeric(df.no.na[row.idx1, ])
                                      row.point  <- as.numeric(df.no.na[row.idx2, ])
                                      
                                      know.dist2 <- sum((this.point - row.point)^2)
                                      
                                      know.dist[row.idx1, row.idx2] <<- know.dist2
                                      know.dist[row.idx2, row.idx1] <<- know.dist2
                                      return(know.dist2)
                                    }))
  #self distances to be 0
  know.dist[is.na(know.dist)][] <<- 0
  
  for(impute.this in na.cols)
  {
    na.row.nums <- which(is.na(in.df[[impute.this]]))
    
    system.time( imputed.vals <- sapply(na.row.nums, 
                                        function(this.row.number)
                                        {
                                          this.point <- df.no.na[this.row.number, ]
                                          eucl.dist.squared <- know.dist[this.row.number, ]
                                          spead <- sd(sqrt(eucl.dist.squared))
                                          weights <- exp(-eucl.dist.squared/(2 * spead^2))
                                          
                                          avbl.values <- scaled.stnd.df[[impute.this]]
                                          
                                          #SHould we redo outliers
                                          
                                          weight.norm <- weights/sum(weights)
                                          sum(avbl.values * weight.norm)
                                        }))
    in.df[na.row.nums, impute.this] <- imputed.vals
  }
  in.df
}
```



#### Modelling part
```{r}
dates <- data.frame("dates" = scaled.stnd.df$dates)
timing <- system.time( imputed <- imput.fun(in.df = scaled.stnd.df[,-1]))
print(timing)
colnames(imputed) <- colnames(scaled.stnd.df)[-1]
```
















