---
title: "Recession-Forecast"
author: "Jai Vrat Singh"
date: "23/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r warning=FALSE, message=FALSE}
library(xts)
library(PerformanceAnalytics)
library(tseries)
library(forecast)
library(knitr)
library(kableExtra)
library(ggplot2)
```

Set working directory

```{r}
setwd("/Users/jvsingh/work/github/R-codes/recession")

direct.df <- read.csv(file = "./data/2019-08.csv", header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
direct.df <- direct.df[-1, ] #The first row is transform etc..

#Date is in month/date/year format
direct.df$sasdate <- as.Date(direct.df$sasdate, "%m/%d/%Y")

#get into time series as it will be earier to work with
direct.df.xts <- xts(direct.df[,-1], direct.df$sasdate)

#Replace missing values by the latest observation. Please note that there may be missing values in "real" decision scenarios which may not be 
#caught by one time experiment. ie in case if we are doing real backtests
direct.df.xts <- zoo::na.locf(direct.df.xts)
```


```{r}
plot.xts(direct.df.xts, legend.loc = "bottomright")
```


As seen from the plot many series seem to ne non-stationary. We need to check for seasonality as well.

```{r}
#install.packages("visdat")
library(visdat)
vis_dat(data.frame(direct.df.xts))
```

```{r}
 #apply(direct.df.xts, 1, function(elem) sum(is.na(elem)))
```
Seems that there are many missing till 1959-12-01. Let us drop them

```{r}
vis_dat(data.frame(direct.df.xts))

direct.df.xts <- window(direct.df.xts,start = as.Date("1959-12-01") + 1)
vis_dat(data.frame(direct.df.xts))
```



```{r}

head(direct.df.xts)

#For all columns let us create yoy, mom, qq, semi

#diff(direct.df.xts[, "RPI"], lag = 1, differences = 1, arithmetic = TRUE, log = FALSE, na.pad = TRUE, ...)


diff.periodic <- function(name){
 this.xts  <- direct.df.xts[, name]
 yoy <- diff(direct.df.xts[,name], lag = 12, 
             differences = 1, 
             arithmetic = TRUE, log = FALSE, na.pad = TRUE)
 names(yoy) <- sprintf("%s_yoy", name)
 
 qoq <- diff(direct.df.xts[,name], lag = 3, 
             differences = 1, 
             arithmetic = TRUE, log = FALSE, na.pad = TRUE)
 names(qoq) <- sprintf("%s_qoq", name)
 
 sem <- diff(direct.df.xts[,name], lag = 6, 
             differences = 1, 
             arithmetic = TRUE, log = FALSE, na.pad = TRUE)
 names(sem) <- sprintf("%s_sem", name)
 
 mom <- diff(direct.df.xts[,name], lag = 1, 
             differences = 1, 
             arithmetic = TRUE, log = FALSE, na.pad = TRUE)
 names(mom) <- sprintf("%s_mom", name)
 
 list(yoy = yoy, sem=sem, qoq=qoq, mom=mom)
}

res     <- sapply(names(direct.df.xts), diff.periodic, simplify = FALSE)
res.xts <- xts(rep(0, length(index(direct.df.xts))), index(direct.df.xts))
for(name in names(direct.df.xts))
{
 # print(class(res[[name]]))
  this.xts <- do.call(merge.xts, res[[name]])
  res.xts  <- merge.xts(res.xts,this.xts)
}
res.xts <- res.xts[,-1]

```



##### Check for seasonality


```{r}
is.any.seasonality <- sapply(names(res.xts), 
                             function(series.name){
                                res.tmp <- try(stl(res.xts[, series.name], s.window = "periodic"), silent = TRUE)
                                ifelse(class(res.tmp) == "try-error", FALSE, TRUE)
                             })
if(any(is.any.seasonality))
{
  warning("Some series are seasonal")
}
```



#### Do we need to make them stationary
```{r}
suggest.diff <- sapply(names(res.xts), 
                             function(series.name){
                                forecast::ndiffs(res.xts[, series.name], alpha = 0.05, max.d = 10)
                             })
#Lets ignore it for a moment
```


#### Lets have look at NA's

```{r}
library(visdat)
vis_dat(data.frame(res.xts))
```

BEcause of yoy, many data till 1960-12-01 are NA. Lets drop


```{r}
res.xts <- window(res.xts, start = as.Date("1960-12-01") + 1)
#Also remove last 3 rows and some data may not have arrived
last.rows <- (dim(res.xts)[1]-3):(dim(res.xts)[1])
res.xts <- res.xts[-last.rows, ]
vis_dat(data.frame(res.xts))
```


```{r}
sapply(res.xts, function(x) sum(is.na(x)))
sort(sapply(res.xts, function(x) sum(is.na(x))))
```

There are columns such as 

Rather than imputing, lets check if we can drop them as they may be highly explainable by others.

```{r}
drop.check.cols <- c("ANDENOx", "UMCSENTx", "VXOCLSx", "TWEXMMTH", "ACOGNO")
drop.col.names <- apply(expand.grid(drop.check.cols, c("_mom", "_sem", "_qoq", "_yoy")), 1, function(e) paste(e, collapse = ""))

spliced.df <- as.data.frame(res.xts[, names(res.xts)[!names(res.xts) %in% drop.col.names]])
dropped.df <- as.data.frame(res.xts[, drop.col.names])

### pca of spliced
pca <- prcomp(spliced.df, scale = TRUE, center = TRUE)
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var) * 100, 1)
plot(pca.var.per, type = "l")

pca$rotation

#PCA directions are pca$rotation[,1], pca$rotation[,2]
apply(pca$rotation, 2, function(x) sum(x^2))

means.all <- apply(spliced.df, 2, mean)
sd.all    <- apply(spliced.df, 2, sd)

scaled.centered <-  (spliced.df[1, ] - means.all) / sd.all

#First is data mapped to principal component direction : scaled.centered %*% pca$rotation
#Second is pca$x
as.matrix(scaled.centered) %*% pca$rotation - pca$x[1,]

#lest see of dropped columns are linear expressibly by rest of data, then we can simply drop them
sapply(names(dropped.df), function(col.name ){
  tmp <- data.frame("col" = dropped.df[[col.name]], as.data.frame(pca$x))
  tmp <- tmp[complete.cases(tmp), ]
  summary(lm( col ~ ., data = tmp))$adj.r.squared
})

#ABove Shows TWEXMMTH ANDENOx UMCSENTx may be dropped. But it might be better if we simply replace them by their median values. The algorithm can ignore them if sensitivity not found

tmp <- sapply(names(res.xts) , function(x){ 
                                           vals <- as.numeric(res.xts[, x])
                                           vals[is.na(vals)] <- median(vals, na.rm = TRUE)
                                           vals
                               })
tmp.xts <- xts(as.data.frame(tmp), index(res.xts))
names(tmp.xts) <- names(res.xts)

final.data.xts <- tmp.xts

setwd("~/work/github/R-codes/recession")

rec.df <- read.csv(file = "data/USREC.csv", stringsAsFactors = FALSE, header = TRUE)
rec.df$DATE = as.Date(rec.df$DATE)
rec.df.xts <- xts(rec.df$USREC, rec.df$DATE)

rec.df.adj.xts <- lag(rec.df.xts, -12)
final.data.xts <- merge.xts(rec.df.adj.xts, final.data.xts)
final.data.xts <- final.data.xts[complete.cases(final.data.xts), ]
  
  
#70% train set
till.row  <- floor(0.70 * dim(final.data.xts)[1])
train.idx <- (1:dim(final.data.xts)[1]) < till.row 
train_set <- final.data.xts[train.idx, ]
test_set  <- final.data.xts[!train.idx, ]

train_set

a <- seq(0.1, 0.9, 0.05)
search <- foreach(i = a, .combine = rbind) %dopar% {
  cv <- cv.glmnet(mdlX, mdlY, family = "binomial", nfold = 10, type.measure = "deviance", paralle = TRUE, alpha = i)
  data.frame(cvm = cv$cvm[cv$lambda == cv$lambda.1se], lambda.1se = cv$lambda.1se, alpha = i)
}
cv3 <- search[search$cvm == min(search$cvm), ]
md3 <- glmnet(mdlX, mdlY, family = "binomial", lambda = cv3$lambda.1se, alpha = cv3$alpha)
coef(md3)
#(Intercept) -1.434700e+00
#AGE         -8.426525e-04
#ACADMOS      .           
#ADEPCNT      .           
#MAJORDRG     6.276924e-02
#MINORDRG     .           
#OWNRENT     -2.780958e-02
#INCOME      -1.305118e-04
#SELFEMPL     .           
#INCPER      -2.085349e-06
#EXP_INC      .           
#SPENDING     .           
#LOGSPEND    -9.992808e-02
roc(newY, as.numeric(predict(md3, newX, type = "response")))
#Area under the curve: 0.6449


```




