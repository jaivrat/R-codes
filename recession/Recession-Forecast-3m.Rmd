---
title: "Recession-Forecast-3m"
author: "Jai Vrat Singh"
date: "25/01/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r warning=FALSE, message=FALSE}
rm(list=ls())
library(xts)
library(PerformanceAnalytics)
library(tseries)
library(forecast)
library(knitr)
library(kableExtra)
library(ggplot2)
library(glmnet)
library(caret)
library(visdat)
library(pracma) #For vector products
```

Set working directory

```{r}
setwd("/Users/jvsingh/work/github/R-codes/recession")

#10Y -3M
ADDSPREAD <- TRUE
#yoy/mom on pct changes or direct difference
PCT       <- FALSE
#--- Forecast tenor: we want to say recession(at t+Forecast tenor) = f(data at t)
FORECAST.TENOR = 12



rec.df      <- read.csv(file = "data/USREC.csv", stringsAsFactors = FALSE, header = TRUE)
rec.df$DATE <- as.Date(rec.df$DATE)
rec.df.xts  <- xts(rec.df$USREC, rec.df$DATE)
names(rec.df.xts) <- "USREC"

direct.df <- read.csv(file = "./data/2019-11.csv", header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
direct.df <- direct.df[-1, ] #The first row is transform etc..

#Date is in month/date/year format
direct.df$sasdate <- as.Date(direct.df$sasdate, "%m/%d/%Y")

#get into time series as it will be earier to work with
direct.df.xts <- xts(direct.df[,-1], direct.df$sasdate)


#Add another row of GS10-TB3MS
if(ADDSPREAD)
{
  x.xts <-  direct.df.xts[, c("TB3MS", "GS10")]
bond.eqv <- function(discount)
{
  100 * (365 * discount/100)/(360 - 91 * discount/100)
}

x.xts$TB3MSBEQ       <- bond.eqv(as.numeric(x.xts$TB3MS))
x.xts$GS10_TB3MS     <- x.xts$GS10 - x.xts$TB3MSBEQ
old.names            <- names(direct.df.xts)
direct.df.xts        <- merge.xts(direct.df.xts, x.xts$GS10_TB3MS, all = TRUE, check.names = FALSE)
names(direct.df.xts) <- c(old.names, "GS10_TB3MS")
}


#Replace missing values by the latest observation. Please note that there may be missing values in "real" decision scenarios which may not be 
#caught by one time experiment. ie in case if we are doing real backtests
direct.df.xts <- zoo::na.locf(direct.df.xts)

#plot.xts(direct.df.xts[, c("TB3MS", "GS10", "GS10_TB3MS")], legend.loc = "topright")
```


```{r}
plot.xts(direct.df.xts, legend.loc = "bottomright")
```


As seen from the plot many series seem to be non-stationary. We need to check for seasonality and stationarity.

```{r}
#install.packages("visdat")
vis_dat(data.frame(direct.df.xts))
```

Seems that there are many missing till 1959-12-01. Let us drop these initial rows

```{r}
direct.df.xts <- window(direct.df.xts,start = as.Date("1959-12-01") + 1)
#vis_dat(data.frame(direct.df.xts))
```



```{r}
#head(direct.df.xts[,1:10])
#For all columns let us create yoy, mom, qq, semi


#------------------------ FUNCTION ---------------------------------#
#-- diff.periodic:   Calculate Periodic Differences
#-- input: col name, and pct/non.pct
#-------------------------------------------------------------------#
diff.periodic <- function(name, pct.diff){
     #print(sprintf("diff.periodic, name = %s", name))
                   this.xts  <- direct.df.xts[, name]
                   yoy <- diff(direct.df.xts[,name], lag = 12, 
                               differences = 1, 
                               arithmetic = TRUE, log = FALSE, na.pad = TRUE)
                   if(pct.diff)
                   {
                     yoy <- yoy/lag(direct.df.xts[,name], 12)
                   }
                   names(yoy) <- sprintf("%s_yoy", name)
                   
                   qoq <- diff(direct.df.xts[,name], lag = 3, 
                               differences = 1, 
                               arithmetic = TRUE, log = FALSE, na.pad = TRUE)
                   names(qoq) <- sprintf("%s_qoq", name)
                   if(pct.diff)
                   {
                     qoq <- qoq/lag(direct.df.xts[,name], 3)
                   }
                   
                   sem <- diff(direct.df.xts[,name], lag = 6, 
                               differences = 1, 
                               arithmetic = TRUE, log = FALSE, na.pad = TRUE)
                   if(pct.diff)
                   {
                     sem <- sem/lag(direct.df.xts[,name], 6)
                   }
                   names(sem) <- sprintf("%s_sem", name)
                   
                   
                   mom <- diff(direct.df.xts[,name], lag = 1, 
                               differences = 1, 
                               arithmetic = TRUE, log = FALSE, na.pad = TRUE)
                   if(pct.diff)
                   {
                     mom <- mom/lag(direct.df.xts[,name], 1)
                   }
                   names(mom) <- sprintf("%s_mom", name)
                   
                   list(yoy = yoy, sem=sem, qoq=qoq, mom=mom)
}



get.augmented.data <- function(direct.df.xts)
{
  res     <- sapply(names(direct.df.xts), function(name) diff.periodic(name = name, pct.diff = PCT), simplify = FALSE)

  res.xts <- NULL
  for(name in names(direct.df.xts))
  {
    # print(class(res[[name]]))
    if(is.null(res.xts))
    {
      res.xts <- direct.df.xts[ , name]
      names(res.xts) <- name
    } else {
      assign.names    <- c(names(res.xts), name)
      res.xts         <- merge.xts(res.xts, direct.df.xts[ , name])
      names(res.xts)  <- assign.names
    }
      this.xts        <- do.call(merge.xts, res[[name]])
      names(this.xts) <- as.character(unlist(lapply(res[[name]], names)))
      
      assign.names    <- c(names(res.xts), names(this.xts))
      res.xts         <- merge.xts(res.xts,this.xts)
      names(res.xts)  <- assign.names
  }
  res.xts
}

res.xts <- get.augmented.data(direct.df.xts)

```

##### Vizualize the enhanced dataset
```{r}
#vis_dat(data.frame(coredata(res.xts)))
```

##### Unavailable data

Let us drop data which have more than 2% NA and also drop initial rows for which we hve implied NA's because of yoy/mom/sem transformations.
```{r}
x       <- apply(res.xts, 2, function(x) sum(is.na(x))/length(x))
res.xts <- res.xts[, names(x[x<0.02])]
#vis_dat(data.frame(coredata(res.xts)))
```


##### Check for seasonality
```{r}
is.any.seasonality <- sapply(names(res.xts), 
                             function(series.name){
                                res.tmp <- try(stl(res.xts[, series.name], s.window = "periodic"), silent = TRUE)
                                ifelse(class(res.tmp) == "try-error", FALSE, TRUE)
                             })
if(any(is.any.seasonality))
{
  warning("Some series are seasonal")
}
```

We find there is no seasonality issues in the transformed data


#### Check for Stationarity

Check for stationarity and drop those columns which as non-stationary

```{r}
#--------------------------------------------------------
#Function : p.values of adf.test. If pvalue <= 0.05 then stationary
#--------------------------------------------------------
testStationarity <- function(col.name, in.time.series.xts)
{
   this.ser <- in.time.series.xts[, col.name]
   adf.test(this.ser[!is.na(this.ser)], alternative = "stationary")$p.value
}

suppressWarnings(test.pvals <- sapply(names(res.xts), 
                                      function(col.name){
                                        #print(sprintf("Checking stationarity for %s", col.name))
                                        testStationarity(col.name = col.name, in.time.series.xts = res.xts)
                                        }))
# Non-stationary
#names(test.pvals)[test.pvals > 0.05]
#rop those columns which as non-stationary
res.xts <- res.xts[ ,names(test.pvals)[test.pvals <= 0.05] ]
```

#### Do we need to make them stationary (We have dropped non stationary above)
```{r}
# suggest.diff <- sapply(names(res.xts), 
#                              function(series.name){
#                                 forecast::ndiffs(res.xts[, series.name], alpha = 0.05, max.d = 10)
#                              })
# 
# suggest.diff.on.orig <- sapply(names(direct.df.xts), 
#                              function(series.name){
#                                 forecast::ndiffs(direct.df.xts[, series.name], alpha = 0.05, max.d = 10)
#                              })
# 
# stationarized <- sapply( direct.df.xts, function(in.xts){
#                                         #--function
#                                         this.lag <- suggest.diff.on.orig[as.character(names(in.xts))]
#                                         if(this.lag > 0)
#                                         {
#                                           return(diff.xts(in.xts, this.lag))
#                                         } else {
#                                           return(in.xts)
#                                         }
#                                         }, simplify = FALSE)
# for(i in 1:length(stationarized))
# {
#   res.xts <- merge.xts(res.xts, stationarized[[i]], check.names = FALSE)
# }
#Lets ignore it for a moment
```


#### Lets have look at NA's
```{r}
#vis_dat(data.frame(res.xts))
```



Because of yoy, many data till 1960-12-01 are NA. 

```{r}
res.xts   <- window(res.xts, start = as.Date("1960-12-01") + 1)
#Also Remove last rows which may be incomplete
res.xts   <- res.xts[complete.cases(res.xts), ]
vis_dat(data.frame(res.xts))
```

Till this point we have only **stationary** and **complete** data series from  **1961-01-01** to **2019-11-01**.

## FINAL ANALYSIS

We begin here our final analysis on **res.xts** which contains  **stationary** and **complete** data series from  **1961-01-01** to **2019-11-01**.

Explore how many recessions for data we have in FredMD

```{r}
plot.xts(window(rec.df.xts, start =  first(index(res.xts)), end =  last(index(res.xts))))
```


We see that we have 8 recessions in before 2000.  We can separate training/test data into pre-yr2000 and post-yr2000 respectively. 

```{r}
#-------------- Add recession ---------------------#
#Inputs : The X data and the recession time series
#         Adjusts the recession data by the forecast tenor and
#         appends to 
add.recession.data <- function(res.xts, rec.df.xts)
{
  tmp.res.xts.names     <- names(res.xts); tmp.rec.df.xts.names <- names(rec.df.xts)
  final.data.xts        <- merge.xts(lag.xts(rec.df.xts, k = -FORECAST.TENOR), res.xts)
  names(final.data.xts) <- c(tmp.rec.df.xts.names, tmp.res.xts.names)
  final.data.xts[complete.cases(final.data.xts), ]
}

#-- Only complete Cases
final.data.xts        <- add.recession.data(res.xts, rec.df.xts)


#pca$rotation
#PCA directions are pca$rotation[,1], pca$rotation[,2]
#apply(pca$rotation, 2, function(x) sum(x^2))
#means.all       <- apply(spliced.df, 2, mean)
#sd.all          <- apply(spliced.df, 2, sd)
#scaled.centered <-  (spliced.df[1, ] - means.all)/sd.all

#First is data mapped to principal component direction : scaled.centered %*% pca$rotation
#Second is pca$x
#as.matrix(scaled.centered) %*% pca$rotation - pca$x[1,]
#lest see of dropped columns are linear expressibly by rest of data, then we can simply drop them
#sapply(names(dropped.df), function(col.name ){
#  tmp <- data.frame("col" = dropped.df[[col.name]], as.data.frame(pca$x))
#  tmp <- tmp[complete.cases(tmp), ]
#  summary(lm( col ~ ., data = tmp))$adj.r.squared
#})


#--- Split train and test
train.boundary.date <- as.Date("1999-01-31")
train_set.x <- window(final.data.xts[,-1], end = train.boundary.date)
train_set.y <- window(final.data.xts[, 1], end = train.boundary.date)
test_set.x  <- window(final.data.xts[,-1], start = train.boundary.date + 1)
test_set.y  <- window(final.data.xts[, 1], start = train.boundary.date + 1)

#--- Scalings of train
mu.train  <- apply(train_set.x, 2, mean)
sd.train  <- apply(train_set.x, 2, sd)
```


```{r}
train_set.x.scaled <- scale(train_set.x, center = mu.train, scale = sd.train)

mdlY = matrix(train_set.y$USREC, ncol = 1)
mdlX = train_set.x.scaled

set.seed(42)
cv_10  = trainControl(method = "cv", number = 10)
cv_5   = trainControl(method = "cv", number = 5)

dat.df <- data.frame("USREC" = as.factor(train_set.y$USREC), train_set.x.scaled)


hit_elnet = train(
  USREC ~ ., data = dat.df,
  method = "glmnet",
  trControl = cv_5
)

hit_elnet$bestTune
hit_elnet$results

fit.glm <- glmnet(x = mdlX, y = as.matrix(train_set.y$USREC, ncol = 1), 
                  family = "binomial",
                  alpha = hit_elnet$bestTune["alpha"], 
                  lambda =  hit_elnet$bestTune["lambda"],
                  intercept=TRUE, standardize = FALSE)
summary(fit.glm)
#fit.glm$beta
#fit.glm$a0
#plot(fit.glm)
#coef(fit.glm)
```

```{r}
#ON Train Set 
pred.prob.train <- predict(fit.glm, as.matrix(train_set.x.scaled), type = "response")
resp.train.xts  <- xts(pred.prob.train, index(train_set.x.scaled))
resp.train.xts  <- merge.xts(resp.train.xts, train_set.y$USREC)
plot(resp.train.xts)
```

```{r}
#Prediction on Test Set
test_set.x.scaled <- scale(test_set.x, center = mu.train, scale = sd.train)
pred.prob.test    <- predict(fit.glm, as.matrix(test_set.x.scaled), type = "response")
resp.test.xts     <- xts(pred.prob.test, index(test_set.x.scaled))
resp.test.xts     <- merge.xts(resp.test.xts, test_set.y$USREC)
plot(resp.test.xts)
```

```{r}
# Roc curve: Test Set
library(ROCR)
pred.test <- prediction(predictions = pred.prob.test, labels = test_set.y$USREC) 
perf.test <- performance(pred.test,"tpr","fpr")
plot(perf.test,colorize=TRUE)
```

```{r}
#AUC: Test set
performance(pred.test,measure = "auc")@y.values[[1]]
```

```{r}
#-- Kalman Filter
library(KFAS)
model <- SSModel(res.xts$RPI_yoy ~ SSMtrend(1, Q = 0.01), H = 0.01)
out   <- KFS(model)

temp.xts <- xts(as.numeric(out$att) , index(res.xts$RPI_yoy))
plot.xts(merge.xts(temp.xts, res.xts$RPI_yoy))
```


```{r}
# Classification Tree with rpart
library(rpart)
#
train.df <- data.frame(USREC = train_set.y$USREC, as.data.frame(train_set.x.scaled), check.names = FALSE)

# grow tree
fit.rpart <- rpart(USREC ~ ., method="class", data=train.df)

printcp(fit.rpart) # display the results
plotcp(fit.rpart) # visualize cross-validation results
summary(fit.rpart) # detailed summary of splits

# plot tree
plot(fit.rpart, uniform=TRUE,
   main="Classification Tree for Kyphosis")
text(fit.rpart, use.n=TRUE, all=TRUE, cex=.8)

# create attractive postscript plot of tree
#post(fit, file = "~/work/github/R-codes/recession/dats/tree.ps",
#   title = "Classification Tree for Kyphosis")


# prune the tree
pfit <- prune(fit.rpart, cp=   fit.rpart$cptable[which.min(fit.rpart$cptable[,"xerror"]),"CP"])

# plot the pruned tree
plot(pfit, uniform=TRUE,
   main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
```

```{r}
#Prediction on Train Set
pred.rpart.train    <- predict(pfit, train.df, type = "prob")
resp.train.rpart.xts     <- xts(pred.rpart.train[,2], index(train_set.x))
resp.train.rpart.xts     <- merge.xts(resp.train.rpart.xts, train_set.y$USREC)
plot.xts(resp.train.rpart.xts,  legend.loc = "topright")

#Prediction on Test Set
test.df <- data.frame(USREC = test_set.y$USREC, as.data.frame(test_set.x.scaled), check.names = FALSE)
pred.rpart.test    <- predict(pfit,test.df , type = "prob")
resp.test.rpart.xts     <- xts(pred.rpart.test[,2], index(test_set.y))
resp.test.rpart.xts     <- merge.xts(resp.test.rpart.xts, test_set.y$USREC)
plot.xts(resp.test.rpart.xts, legend.loc = "topright")
```


#### Logistic regression based

1. From the training data identify which variables are highly explaining the 

```{r}

logistic.summary <- function(var.name)
{
#var.name <- names(train_set.x.scaled)[1]
      temp.df <- data.frame(USREC  = coredata(train_set.y$USREC), var.name = as.numeric(train_set.x.scaled[, var.name]))
      tmp.logistic.fit <- glm(USREC ~ ., data = temp.df, family = "binomial")
      sum.res <- summary(tmp.logistic.fit)
      p.val <- sum.res$coefficients[, "Pr(>|z|)"]["var.name"]
      as.numeric(p.val)
}

logistic.summary(names(train_set.x.scaled)[1])
res.lm <- sapply(names(train_set.x.scaled), logistic.summary)


#Statistically significant columns
stat.sig.cols <- names(res.lm[res.lm < 0.05])

library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest

# for reproduciblity
set.seed(123)


temp.train.df = data.frame(USREC = as.factor(train_set.y$USREC), as.data.frame(train_set.x.scaled[, stat.sig.cols]))
# default RF model
m1 <- randomForest(
  formula = USREC ~ .,
  data    = temp.train.df
)

plot(m1)


# Variable Importance
varImpPlot(m1,  
           sort = T,
           n.var=10,
           main="Top 10 - Variable Importance")


# Predicting response variable
rf.pred.train.response = predict(m1 , temp.train.df)
rf.pred.train.response.xts <- xts(as.numeric(levels(rf.pred.train.response)[as.numeric(rf.pred.train.response)]), index(train_set.y))
names(rf.pred.train.response.xts) <- "train.pred.rf"
#merge.xts(rf.pred.train.response.xts, train_set.y)
plot.xts(merge.xts(rf.pred.train.response.xts, train_set.y), legend.loc = "topright")


# Predicting on test set 
temp.test.df = data.frame(USREC = as.factor(test_set.y$USREC), #DO not need this column though
                          as.data.frame(test_set.x.scaled[, stat.sig.cols]))
rf.pred.test.response = predict(m1 , temp.test.df)
rf.pred.test.response.xts <- xts(as.numeric(levels(rf.pred.test.response)[as.numeric(rf.pred.test.response)]), index(test_set.y))
names(rf.pred.test.response.xts) <- "test.pred.rf"
plot.xts(merge.xts(rf.pred.test.response.xts, test_set.y), legend.loc = "topright")

```


#### Lets handpick columns
```{r}
#m1$importance[order(m1$importance), , drop = FALSE]

#Statistically significant columns
chosen.cols <- last(stat.sig.cols, 200)


#Let us do PCA of  selected columns
#We have got scales 
pca.train         <- prcomp(train_set.x.scaled[,chosen.cols], scale = FALSE, center = FALSE) 
pca.train.var     <- pca.train$sdev^2
pca.var.per       <- pca.train.var/sum(pca.train.var)

#Min components needed
VARIANCE.EXPLAIN.THESH = 0.90
num.comp <- min(which(cumsum(pca.var.per) >= VARIANCE.EXPLAIN.THESH))

print(sprintf("#Components explaining %s = %s, total features = %s", VARIANCE.EXPLAIN.THESH,num.comp , length(pca.var.per)))
barplot(round(pca.train.var/sum(pca.train.var) * 100, 1), xlab = "principal-component", ylab="pct. variation")

train_set.x.pca <- (train_set.x.scaled[,chosen.cols] %*% pca.train$rotation)[, 1:num.comp]



dat.df.pca    <- data.frame("USREC" = as.factor(train_set.y$USREC), train_set.x.pca)
pca.fit.logit <- glm(USREC ~ ., data = dat.df.pca, family = "binomial")
pca.fit.logit <- glm(USREC ~ ., data = dat.df.pca[, 1:5], family = "binomial")

#ON Train Set 
pred.prob.train.pca <- predict(pca.fit.logit, dat.df.pca, type = "response")
resp.train.pca.xts  <- xts(pred.prob.train.pca, index(train_set.x.scaled))
resp.train.pca.xts      <- merge.xts(resp.train.pca.xts, train_set.y$USREC)
plot(resp.train.pca.xts)


#Prediction on Test Set
test_set.x.pca <- data.frame((test_set.x.scaled[,chosen.cols] %*% pca.train$rotation)[, 1:num.comp])
pred.prob.test.pca  <- predict(pca.fit.logit, test_set.x.pca, type = "response")
resp.test.pca.xts   <- xts(pred.prob.test.pca, index(test_set.x.scaled))
resp.test.pca.xts   <- merge.xts(resp.test.pca.xts, test_set.y$USREC)
plot(resp.test.pca.xts)
```

