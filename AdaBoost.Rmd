---
title: "Adaboost"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Adaboost

Prepared from Qize (Chase) Le's  https://qizeresearch.wordpress.com/2013/12/05/short-example-for-adaboost/

AdaBoost (adaptive boost) algorithm is another ensemble classification technology in data mining. Let's suppose the y value (yes or not) is (+1, -1)

1. first find a week classifier with $N$ observations (assign each observation equal weight $1/N$)

2. calculate errors between predicted $y$ values and real $y$ values use the errors as information to reweigh each observation (the total weight is set to be 1); 

3. repeat this process T times; 

4. finally output the sign of a combination of all classifier outputs. 

The reason for using AdaBoost algorithm is that: we could not find a strong classifier some time at the beginning. Normally, in this case, the most important things we need to do is to brainstorm some new features, which really captures some insight of (people's) behaviors. However, in case we cannot figure out new features, we had to use AdaBoost to somehow leverage our current knowledge, and build a strong classifier.


Let's look at the algorithm of AdaBoost: (suppose $Y=\{-1, +1\}$)

* **Step 1**: suppose we have $N$ observations, we weight each observation with $1/N$ weight ($W_i$). 

$D_i=\frac{W_i}{\sum W_i}$. In other word, $D_i$ is the normalized weight.

* **Step 2**: To the following sub-steps for $t=1$ to $T$, repeat $T$ times:
    + **Sub-step2.1**: 
    
    select a base classifier $h$ (can be CART, Logit, probit, etc), and train data with the given weight
    + **Sub-step2.2**: 
    Typically, we recognize the err of the classifier:
    
    $Err(t)= \sum D_i * I(y_i != h_t(x_i))$. In other words, we sum $D_i$ for all the observations with the predicted value not equal to the true value. $a_t = 1/2 * ln(\frac{1-Err(t)}{Err(t)})$
    + **Sub-step2.3**: 
    
    update Weight $W_i(t+1) = D_i(t) * exp(-a_t * Y_i * h_t(X_i))$
    + **Sub-step2.4** : normalized weight for each observation: 
    $Di(t+1)=Wi(t+1)/Sum(Wi(t+1))$. In other word, Di(t+1) is normalized Wi(t+1) in t+t step.


* **Step 3**: H(x)=sign(sum(a(t)*ht(x))). t from 1 to T.

In R, the adaboost package (named adabag) can be downloaded and install. This package implements the Freund and Schapire's Adaboost.M1 algorithm. The 'weak' classifier used in this algorithm is CART. Notice that before install adabag package, you first need: rpart, mlbench, caret, lattice, reshape2, plyr, cluster, foreach packages. Some of them are pre-install by default in R. The Lack of system integration is a drawback for open source software.


```{r}
library(adabag)
adadata <- read.csv("/Users/jvsingh/work/github/R-codes/data/bank/bank-full.csv", header=TRUE,sep=";")
adaboost<-boosting(y ~ ., data=adadata, boos=TRUE, mfinal=20, coeflearn='Breiman')
```


Here, 

* **mfinal**=20 indicates the times of repeated process is 20. 
* **coeflearn**="Breiman" indicates using M1 algorithm proposed by Breiman. Other options are "Freund" and "Zhu". The differences are in the calculation of a(t) . 

* **errorevol** function can be used to show the evolution of error during the boosting process.

* **predict** function can be used to predict new data based on the existing adaboost trees. 

In reality, we can use adaboost\$trees to get all 20 trees, and also use adaboost$weights to get all weights, then calculate the predict result for new data by ourselves (saving time).


```{r pressure, echo=FALSE}
summary(adaboost)

adaboost$trees
adaboost$weights
adaboost$importance

errorevol(adaboost,adadata)
predict(adaboost,adadata)

t1<-adaboost$trees[[1]]
#library(tree)
#plot(t1)
#text(t1,pretty=0)
```
